{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # 优化 GPU 线程分配\n",
    "os.environ['TF_ENABLE_GPU_GARBAGE_COLLECTION'] = 'true'\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(16)\n",
    "# 启用混合精度（FP16）\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stock_features.csv', parse_dates=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "target ='close'\n",
    "features = df.drop(columns=['date', target]).values\n",
    "X = features.copy()\n",
    "y = df[target].copy().values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X)\n",
    "y_train_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "\n",
    "def create_sequneces(X, y, time_steps=24):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数据增强\"\"\"\n",
    "def moving_average_smoothing(series, window_size=3):\n",
    "    smoothed_data = np.empty_like(series)  # 创建一个与原始数据形状相同的空数组\n",
    "    for col in range(series.shape[1]):\n",
    "        # 对每一列(每个特征)进行平滑处理\n",
    "        smoothed_data[:,col] = np.convolve(series[:,col], np.ones(window_size)/window_size, mode='same')\n",
    "    return smoothed_data\n",
    "\n",
    "def random_noise(data, noise_factor=0.01):\n",
    "    \"\"\"随机噪声\"\"\"\n",
    "    noise = noise_factor + np.random.randn(*data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def time_series_shift(series, shift_range=5):\n",
    "    \"\"\"时间序列平移\"\"\"\n",
    "    shift = np.random.randint(-shift_range, shift_range + 1)\n",
    "    return np.roll(series, shift, axis=0)\n",
    "\n",
    "def data_augmentation(X, y, num_augmentations=5):\n",
    "    augmented_X, augmented_y = [], []\n",
    "    for i in range(len(X)):\n",
    "        # 移动平均平滑\n",
    "        augmented_X.append(X[i])\n",
    "        augmented_y.append(y[i])\n",
    "        for _ in range(num_augmentations):\n",
    "            # 增强方法1 平滑处理\n",
    "            X_smooth = moving_average_smoothing(X[i])\n",
    "            # 增加方法2 添加噪声\n",
    "            X_noise = random_noise(X_smooth)\n",
    "            # 增加方法3 时间偏移\n",
    "            X_shift = time_series_shift(X_noise)\n",
    "            augmented_X.append(X_shift)\n",
    "            augmented_y.append(y[i])\n",
    "    return np.array(augmented_X), np.array(augmented_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, fold =1):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(history.history['loss'], label=f'Fold {fold + 1} - Training Loss', color='blue', linewidth=2)\n",
    "    # 绘制验证损失曲线（如果有验证集）\n",
    "    plt.plot(history.history['val_loss'], label=f'Fold {fold + 1} - Validation Loss', color='orange', linewidth=2)\n",
    "\n",
    "    plt.title(f'Fold {fold + 1} -Model Training History')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, batch_size=128, shuffle=True):\n",
    "    # 创建 TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.cache()  # 缓存到内存\n",
    "    # 优化流水线（顺序很重要！）\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X)*2, reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # 自动预加载\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size: 10\n",
      "划分数据集...\n"
     ]
    }
   ],
   "source": [
    "window_sizes = [10, 30, 60, 90, 120]\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'R2']\n",
    "\n",
    "results = {window_size: {metric: [] for metric in metrics} for window_size in window_sizes}\n",
    "\n",
    "def train_windows(X, y, window_size=24, stride=1):\n",
    "    \"\"\"\n",
    "    将时间序列数据划分为多个窗口，每个窗口包含过去window_size个时间步的数据和下一个时间步的标签。\n",
    "    \"\"\"\n",
    "    X_train, y_train = create_sequneces(X_train_scaled, y_train_scaled, window_size)\n",
    "    print(\"划分数据集...\")\n",
    "\n",
    "    X_train_full_augmented, y_train_full_augmented = data_augmentation(X_train, y_train)\n",
    "\n",
    "    print(f\"Origin traning data shape: {X_train.shape}\")\n",
    "    print(f\"Augmented traning data shape: {X_train_full_augmented.shape}\")\n",
    "\n",
    "    print(\"交叉验证...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=30)\n",
    "    result = {metric: [] for metric in metrics}\n",
    "    for fold,(train_index, val_index) in enumerate(tscv.split(X_train_full_augmented)):\n",
    "        print(f\"Fold {fold + 1}:\")\n",
    "        X_train_fold, X_val_fold = X_train_full_augmented[train_index], X_train_full_augmented[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_full_augmented[train_index], y_train_full_augmented[val_index]\n",
    "\n",
    "        print(\"构建LSTM模型...\")\n",
    "        model = Sequential([\n",
    "            LSTM(units=64, \n",
    "                input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=regularizers.l2(0.01)),\n",
    "            Dropout(0.3),\n",
    "            LSTM(units=64, return_sequences=False),\n",
    "            Dropout(0.3),\n",
    "            Dense(units=1)])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate= 0.0001), loss='mean_squared_error', metrics=['mae'])\n",
    "        \n",
    "        model.summary()\n",
    "        # 替换原有数据集创建代码\n",
    "        train_dataset = create_dataset(X_train_fold, y_train_fold, batch_size=32)\n",
    "        val_dataset = create_dataset(X_val_fold, y_val_fold, batch_size=32, shuffle=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        tf.debugging.set_log_device_placement(True)  # 打印设备分配日志\n",
    "\n",
    "        history = model.fit(train_dataset, epochs=20, batch_size=32, validation_data=val_dataset, verbose=1, callbacks=[early_stopping])\n",
    "        plot_history(history=history, fold=fold)\n",
    "\n",
    "        val_predictions = model.predict(X_val_fold)\n",
    "        val_predictions_prices = scaler_y.inverse_transform(val_predictions)\n",
    "        val_real_prices = scaler_y.inverse_transform(y_val_fold.reshape(-1, 1))\n",
    "\n",
    "        mse = mean_squared_error(val_real_prices, val_predictions_prices)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(val_real_prices, val_predictions_prices)\n",
    "        r2 = r2_score(val_real_prices, val_predictions)\n",
    "        result['MSE'].append(mse)\n",
    "        result['RMSE'].append(rmse)\n",
    "        result['MAE'].append(mae)\n",
    "        result['R2'].append(r2)\n",
    "        print(f\"Fold {fold + 1} for window_{window_size} - MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R2: {r2}\")\n",
    "    return result\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"Window Size: {window_size}\")\n",
    "    result = train_windows(X, y, window_size=window_size)\n",
    "    results[window_size] = result\n",
    "\n",
    "averages = {window_size: {metric: np.mean(result[metric]) for metric in result} for window_size, result in results.items()}\n",
    "\n",
    "# 对指标进行对数变换，仅对MSE、RMSE进行操作\n",
    "log_transformed_averages = {window_size: {\n",
    "    \"MSE\": np.log(averages[window_size][\"MSE\"]),\n",
    "    \"RMSE\": np.log(averages[window_size][\"RMSE\"]),\n",
    "    \"MAE\": averages[window_size][\"MAE\"],\n",
    "    \"R2\": averages[window_size][\"R2\"] \n",
    "    } for window_size in results.items()}\n",
    "\n",
    "colors = sns.color_palette(\"Blues\", len(window_sizes))\n",
    "r = np.arange(len(metrics))\n",
    "plt.figure(figsize=(10, 6))\n",
    "barWidth = 0.15 # 每个柱子的宽度\n",
    "for i, (window_size, result) in enumerate(results.items()):\n",
    "    avg_metrics =[log_transformed_averages[metric] for metric in metrics]\n",
    "    bars = plt.bar(r + i * barWidth, avg_metrics, width=barWidth, colors=colors[i], edgecolor='white', label=f'Window Size {window_size}')\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')\n",
    "plt.xlabel(\"Metrics\", fontweight='bold')\n",
    "plt.ylabel(\"log-transformed / Original Value\", fontweight='bold')\n",
    "plt.xticks([r + barWidth * (len(window_sizes) /2 -0.5) for r in np.arange(len(metrics))], metrics)\n",
    "\n",
    "plt.title(\"Log-Transformed Evaluation Metrics for Different Window Sizes\", fontweight='bold')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
