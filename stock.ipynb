{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import talib as ta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "DataFrame = pd.DataFrame\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data:pd.DataFrame):\n",
    "    # 生成技术指标\n",
    "    data['SMA'] = data['close'].rolling(window=20).mean()\n",
    "    data['EMA20'] = data['close'].ewm(span=20, adjust=False).mean()\n",
    "    data['MACD'] = data['close'].ewm(span=12, adjust=False).mean() - data['close'].ewm(span=26, adjust=False).mean()\n",
    "    data['Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    data['MACD_Histogram'] = data['MACD'] - data['Signal']\n",
    "    data['RSI'] = ta.RSI(data['close'], timeperiod=14)\n",
    "    data['UpperBB'],data['MiddleBB'],data['LowerBB'] = ta.BBANDS(data['close'], timeperiod=20)\n",
    "    data['ATR'] = ta.ATR(data['high'], data['low'], data['close'], timeperiod=14)\n",
    "    data['Volatility'] = data['ATR'] / data['close']\n",
    "    sns.pairplot(data[[\"MACD\", \"close\"]], diag_kind=\"kde\")\n",
    "\n",
    "    sns.pairplot(data[[\"RSI\", \"close\"]], diag_kind=\"kde\")\n",
    "    return data.sort_values(by='date', ascending=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data:DataFrame, lookahead = 5):\n",
    "    # 特征工程\n",
    "    for lag in [1, 3, 5]:\n",
    "        data[f'return_lag{lag}'] = data['close'].pct_change(lag)\n",
    "    # 添加技术指标交叉信号\n",
    "    data['MACD_cross'] = np.where(data['MACD'] > data['Signal'], 1, -1)\n",
    "    data['BB_width'] = (data['UpperBB'] - data['LowerBB']) / data['MiddleBB']\n",
    "    # 严格时序特征生成\n",
    "    data['future_return'] = data['close'].pct_change(lookahead).shift(-lookahead -1)\n",
    "\n",
    "    data['log_return'] = np.log(data['close']).diff()\n",
    "    data['volatility_30'] = data['log_return'].rolling(30).std()\n",
    "    # 新增时间序列特征\n",
    "    data['month'] = data.index.month\n",
    "    data['day_of_week'] = data.index.dayofweek\n",
    "\n",
    "    # 特征选择\n",
    "    selected_features = ['RSI', 'MACD','Signal', 'volatility_30',  'UpperBB', 'MiddleBB', 'LowerBB', 'BB_width',\n",
    "                        'return_lag1', 'return_lag3', 'month']\n",
    "    data['return'] = data['close'].pct_change()\n",
    "    # 确保时间对齐\n",
    "    data = data.dropna()\n",
    "    \n",
    "    return  data[selected_features], data['future_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense\n",
    "\n",
    "# from transformer import TransformerEncoder  # 需要自定义或使用现有实现\n",
    "tf.config.list_physical_devices('GPU')\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.attn(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, self.d_model, 2, dtype=tf.float32) * \n",
    "                        (-tf.math.log(10000.0) / self.d_model))\n",
    "        \n",
    "        pe = tf.zeros((1, seq_len, tf.cast(self.d_model, tf.int32)))\n",
    "        pe = tf.Variable(pe)\n",
    "        pe[:, :, 0::2].assign(tf.sin(position * div_term))\n",
    "        pe[:, :, 1::2].assign(tf.cos(position * div_term))\n",
    "        return inputs + pe.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_configs() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"统一管理模型配置\"\"\"\n",
    "    return {\n",
    "        'random_forest': {\n",
    "            'pipeline': Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', RandomForestRegressor())\n",
    "            ]),\n",
    "            'param_grid': {\n",
    "                'regressor__n_estimators': [100, 200],\n",
    "                'regressor__max_depth': [5, 8]\n",
    "            }\n",
    "        },\n",
    "        'xgb': {\n",
    "            'pipeline': Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('regressor', XGBRegressor(\n",
    "                    objective='reg:squarederror', \n",
    "                    n_jobs=-1\n",
    "                ))\n",
    "            ]),\n",
    "            'param_grid': {\n",
    "                'regressor__learning_rate': [0.005, 0.01, 0.05],\n",
    "                'regressor__max_depth': [3, 5, 7],\n",
    "                'regressor__subsample': [0.6, 0.8],\n",
    "                'regressor__colsample_bytree': [0.7, 0.9],\n",
    "                'regressor__n_estimators': [200, 300]\n",
    "            }\n",
    "        },\n",
    "        'lstm': {\n",
    "            'seq_length': 30,\n",
    "            'params': {\n",
    "                'units': 128,   # 增加LSTM神经元单元数\n",
    "                'dropout': 0.2, #随机丢弃输入的某些单元，防止过拟合\n",
    "                'learning_rate': 1e-3,\n",
    "                'batch_size': 64,\n",
    "                'epochs': 100,\n",
    "                \"recurrent_dropout\": 0.2, # 随机丢弃LSTM单元的输出，防止过拟\n",
    "                \"return_sequences\": True, # False, 仅返回最后一个时间步的输出 True 返回每个时间步的输出\n",
    "                # \"den\"\n",
    "            }\n",
    "        },\n",
    "        'transformer': {\n",
    "            'seq_length': 30,\n",
    "            'params': {\n",
    "                'd_model': 64,\n",
    "                'num_heads': 4,\n",
    "                'ff_dim': 128,\n",
    "                'num_layers': 2,\n",
    "                'dropout': 0.1,\n",
    "                'learning_rate': 1e-4,\n",
    "                'batch_size': 32,\n",
    "                'epochs': 200\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"创建时间序列样本\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_lstm_model(input_shape, params):\n",
    "    \"\"\"构建LSTM模型\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(params['units'], input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(params['dropout']),\n",
    "        LSTM(params['units']),\n",
    "        Dropout(params['dropout']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(params['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_transformer_model(input_shape, params):\n",
    "    \"\"\"构建Transformer模型\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # 位置编码层\n",
    "    x = PositionalEncoding(params['d_model'])(inputs)\n",
    "    \n",
    "    # # Transformer编码层\n",
    "    for _ in range(params['num_layers']):\n",
    "        x = TransformerEncoder(params['d_model'], params['num_heads'], params['ff_dim'])(x)\n",
    "    \n",
    "    # 全局平均池化 + 输出层\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(params['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TimeSeriesModel(BaseEstimator):\n",
    "    \"\"\"统一时序模型接口\"\"\"\n",
    "    def __init__(self, model_type='lstm', **params):\n",
    "        self.model_type = model_type\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.val_performance = {}\n",
    "        self.performance = {}\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 数据预处理\n",
    "        seq_length = self.params['seq_length']\n",
    "        X_seq, y_seq = create_sequences(X.values, seq_length)\n",
    "        \n",
    "        # 构建模型\n",
    "        if self.model_type == 'lstm':\n",
    "            self.model = build_lstm_model((seq_length, X.shape[1]), self.params)\n",
    "        elif self.model_type == 'transformer':\n",
    "            self.model = build_transformer_model((seq_length, X.shape[1]), self.params)\n",
    "        \n",
    "        # 训练\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "        self.history = self.model.fit(\n",
    "            X_seq, y_seq,\n",
    "            batch_size=self.params['batch_size'],\n",
    "            epochs=self.params['epochs'],\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.val_performance['Linear'] = self.model.evaluate(single_step_window.val)\n",
    "        self.performance['Linear'] = self.model.evaluate(single_step_window.test, verbose=0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "def cross_validate_model(\n",
    "    model: BaseEstimator,\n",
    "    param_grid: Dict[str, Any],\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    tscv: TimeSeriesSplit\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"通用交叉验证流程\"\"\"\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    return {\n",
    "        'best_model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(model, feature_names):\n",
    "    \"\"\"可视化特征重要性\"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    try:\n",
    "         # 兼容不同模型结构\n",
    "        if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            importance = pd.Series(model.named_steps['regressor'].feature_importances_,index=feature_names).sort_values()\n",
    "\n",
    "        elif hasattr(model.named_steps['regressor'], 'coef_'):\n",
    "            importance = np.abs(model.named_steps['regressor'].coef_)\n",
    "        else:\n",
    "            raise AttributeError(\"模型不支持特征重要性分析\")\n",
    "                \n",
    "        importance.plot(kind='barh', title='Feature Importance')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"特征分析失败: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X, y):\n",
    "    \"\"\"多模型集成训练入口\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=30)\n",
    "    model_configs = get_model_configs()\n",
    "    # estimators = [\n",
    "    #     ('rf', model_configs['random_forest']['pipeline']),\n",
    "    #     ('xgb', model_configs['xgb']['pipeline'])\n",
    "    # ]\n",
    "\n",
    "    # stacking = StackingRegressor(\n",
    "    #     estimators=estimators,\n",
    "    #     final_estimator=XGBRegressor(),\n",
    "    #     cv=tscv\n",
    "    # )\n",
    "    results = {}\n",
    "    for model_name, config in model_configs.items():\n",
    "        if model_name in ['lstm', 'transformer']:\n",
    "            # 深度学习模型训练\n",
    "            model = TimeSeriesModel(model_name, **config['params'])\n",
    "            model.fit(X, y)\n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'history': model.history\n",
    "            }\n",
    "        else: # 普通机器学习模型训练\n",
    "            model_result = cross_validate_model(\n",
    "                config['pipeline'],\n",
    "                config['param_grid'],\n",
    "                X, y, tscv\n",
    "            )\n",
    "            # 保存基准分数到模型对象\n",
    "            model_result['best_model'].baseline_score = model_result['cv_results']['mean_test_score'].max()\n",
    "            # 存储每个模型的最佳结果\n",
    "            results[model_name] = {\n",
    "                'model': model_result['best_model'],\n",
    "                'params': model_result['best_params'],\n",
    "                'cv_score': model_result['cv_results']['mean_test_score'].max()\n",
    "            }\n",
    "            # 新增特征分析\n",
    "            analyze_features(\n",
    "                model_result['best_model'],\n",
    "                feature_names=X.columns.tolist()\n",
    "            )\n",
    "            print(f\"{model_name.upper()} 最佳参数: {model_result['best_params']}\")\n",
    "            print(f\"平均验证得分: {model_result['cv_results']['mean_test_score'].max():.3f}\")\n",
    "    # 为集成模型创建基准分数（取各模型平均）\n",
    "    ensemble_baseline = np.mean([results[model]['cv_score'] for model in results])\n",
    "    ensemble = VotingRegressor(\n",
    "        [(name, results['model']) for name, results in results.items()]\n",
    "    )\n",
    "    ensemble.baseline_score = ensemble_baseline  # 添加基准分数属性\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_model_drift(model, X: pd.DataFrame, y: pd.Series, n_folds: int = 5) -> float:\n",
    "    \"\"\"监控模型性能漂移\n",
    "    Args:\n",
    "        model: 已训练的基准模型\n",
    "        X: 新数据特征\n",
    "        y: 新数据目标值\n",
    "        n_folds: 交叉验证折数\n",
    "    \n",
    "    Returns:\n",
    "        当前分数与基准分数的差值\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_score = cross_val_score(model, X, y, cv=5).mean()\n",
    "        baseline_score = model.baseline_score  # 从保存的基准模型中读取\n",
    "    except Exception as e:\n",
    "        logging.error(f\"漂移检测失败: {str(e)}\")\n",
    "        return np.nan\n",
    "    # 从已保存的基准模型读取历史分数\n",
    "    baseline_score = getattr(model, 'baseline_score', None)\n",
    "    if baseline_score is None:\n",
    "        baseline_score = current_score\n",
    "        logging.warning(\"未找到基准分数，使用当前分数作为基准\")\n",
    "    \n",
    "    return current_score - baseline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor:\n",
    "    \"\"\"模型监控管理器\"\"\"\n",
    "    def __init__(self, baseline_model_path):\n",
    "        self.baseline = joblib.load(baseline_model_path)\n",
    "        self.baseline_score = self.baseline.baseline_score\n",
    "        \n",
    "    def check_drift(self, X_new, y_new, threshold=0.1):\n",
    "        # 添加时间序列有效性验证\n",
    "        if not isinstance(X_new.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"输入数据需要包含有效时间索引\")\n",
    "            \n",
    "        # 添加数据时效性检查（最新数据不应早于3天前）\n",
    "        latest_date = X_new.index.max()\n",
    "        if pd.Timestamp.now() - latest_date > pd.Timedelta(days=3):\n",
    "            logging.warning(\"检测到过期数据：{latest_date}\")\n",
    "\n",
    "        current = cross_val_score(\n",
    "            self.baseline, X_new, y_new,\n",
    "            cv=TimeSeriesSplit(3),\n",
    "            scoring='neg_mean_squared_error'\n",
    "        ).mean()\n",
    "        return (current - self.baseline_score) < threshold\n",
    "    \n",
    "    def plot_performance(self, historical_data):\n",
    "        \"\"\"性能趋势可视化\"\"\"\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(historical_data['dates'], historical_data['scores'], \n",
    "                label='当前表现')\n",
    "        plt.axhline(self.baseline_score, color='r', \n",
    "                   linestyle='--', label='基准表现')\n",
    "        plt.title('Model Performance Monitoring')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Backtest, Strategy\n",
    "\n",
    "class ModelDrivenStrategy(Strategy):\n",
    "    \"\"\"基于模型预测的交易策略\"\"\"\n",
    "    # 策略参数\n",
    "    threshold_entry = 0.02  # 买入阈值\n",
    "    threshold_exit = -0.01  # 卖出阈值\n",
    "    position_size = 0.1  # 仓位比例\n",
    "    \n",
    "    def init(self):\n",
    "        # 加载预训练模型\n",
    "        self.model = joblib.load('stock_predictor.pkl')\n",
    "        \n",
    "        # 初始化指标计算窗口\n",
    "        self.returns = self.I(self.calculate_returns)\n",
    "        \n",
    "    def calculate_returns(self):\n",
    "        \"\"\"实时计算收益率特征\"\"\"\n",
    "        close = self.data.Close.df\n",
    "        return close.pct_change()\n",
    "    \n",
    "    def next(self):\n",
    "        # 仅在有足够历史数据时交易\n",
    "        if len(self.data) < 30:\n",
    "            return\n",
    "        \n",
    "        # 构建特征数据\n",
    "        current_idx = len(self.data) - 1\n",
    "        features = pd.DataFrame({\n",
    "            'RSI': pd.Series(self.data.RSI).iloc[-20:].mean(),\n",
    "            'MACD': self.data['MACD'].iloc[-1],\n",
    "            'volatility': self.data['volatility_30'].iloc[-1],\n",
    "            'BB_width': self.data['BB_width'].iloc[-1],\n",
    "            'return_lag1': self.returns[-1] if len(self.returns) > 0 else 0,\n",
    "            'return_lag3': self.returns[-3:].mean() if len(self.returns) >=3 else 0,\n",
    "            'month': self.data.index[-1].month\n",
    "        }, index=[current_idx])\n",
    "        \n",
    "        # 模型预测\n",
    "        predicted_return = self.model.predict(features)[0]\n",
    "        \n",
    "        # 交易逻辑\n",
    "        if not self.position:\n",
    "            if predicted_return > self.threshold_entry:\n",
    "                self.buy(size=self.position_size)\n",
    "        else:\n",
    "            if predicted_return < self.threshold_exit:\n",
    "                self.position.close()\n",
    "def backtest(data: pd.DataFrame, ensemble, X, y, initial_capital=100000):\n",
    "    \"\"\"执行完整回测流程\"\"\"\n",
    "    # 准备回测数据格式\n",
    "    bt_data = data.rename(columns={\n",
    "        'close': 'Close',\n",
    "        'high': 'High',\n",
    "        'low': 'Low',\n",
    "        'open': 'Open'\n",
    "    }).copy()\n",
    "    # 添加必要字段到回测数据\n",
    "    for col in ['RSI', 'MACD', 'volatility_30', 'BB_width']:\n",
    "        bt_data[col] = data[col]\n",
    "    # 添加衍生特征\n",
    "    bt_data['return_lag1'] = bt_data['Close'].pct_change(1)\n",
    "    bt_data['return_lag3'] = bt_data['Close'].pct_change(3)\n",
    "    bt_data['month'] = bt_data.index.month\n",
    "    # 优化参数（可选）\n",
    "    # stats = bt.optimize(\n",
    "    #     threshold_entry=[0.01, 0.02, 0.03],\n",
    "    #     threshold_exit=[-0.01, -0.02],\n",
    "    #     maximize='Sharpe Ratio'\n",
    "    # )\n",
    "    # 初始化回测引擎\n",
    "    bt = Backtest(\n",
    "        bt_data.dropna(),\n",
    "        ModelDrivenStrategy,\n",
    "        cash=100000,\n",
    "        commission=0.001,  # 考虑交易手续费\n",
    "        exclusive_orders=True\n",
    "    )\n",
    "    stats = bt.run()\n",
    "    bt.plot(filename='backtest_result.html')\n",
    "    \n",
    "    # 关键指标报告\n",
    "    print(f\"夏普比率: {stats['Sharpe Ratio','NaN']:.2f}\")\n",
    "    print(f\"最大回撤: {stats['Max. Drawdown','NaN']:.2%}\")\n",
    "    print(f\"总收益率: {stats['Return [%]','NaN']:.2f}%\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('stock.csv', index_col='date',parse_dates=True)\n",
    "    data = preprocess_data(data)\n",
    "    \n",
    "    features, target = create_features(data)\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    ensemble = train_models(features, target)\n",
    "\n",
    "     # 模拟新数据到来时的监控（示例）\n",
    "    # new_data = pd.read_csv('new_stock_data.csv') \n",
    "    # new_features, _ = create_features(preprocess_data(new_data))\n",
    "    current_drift = track_model_drift(\n",
    "        ensemble,\n",
    "        features.sample(1000),  # 假设取样本数据\n",
    "        target.sample(1000)\n",
    "    )\n",
    "    print(f\"模型性能漂移值: {current_drift:.4f}\")\n",
    "\n",
    "     # 部署集成模型\n",
    "    joblib.dump(ensemble, 'stock_predictor.pkl')\n",
    "\n",
    "    monitor = ModelMonitor('stock_predictor.pkl')\n",
    "    # 4. 执行漂移检测\n",
    "    is_drift = monitor.check_drift(\n",
    "        features.sample(1000, random_state=42),\n",
    "        target.sample(1000, random_state=42),\n",
    "        threshold=-0.15  # 根据业务需求调整阈值\n",
    "    )\n",
    "    \n",
    "    print(f\"是否需要更新模型: {is_drift}\")\n",
    "    # 显示交叉验证结果\n",
    "    # 执行回测\n",
    "    backtest_stats = backtest(data, ensemble, features, target)\n",
    "    \n",
    "    # 策略优化示例（可选）\n",
    "    optimize_params = {\n",
    "        'threshold_entry': [0.015, 0.02, 0.025],\n",
    "        'threshold_exit': [-0.005, -0.01],\n",
    "        'position_size': [0.1, 0.2]\n",
    "    }\n",
    "    optimization_results = backtest_stats.optimize(**optimize_params, maximize='Sharpe Ratio')\n",
    "    print(optimization_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
